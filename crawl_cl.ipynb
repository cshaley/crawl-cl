{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_links_by_state():\n",
    "    \"\"\" This function pulls all of the US State craigslist website cities from craigslist\n",
    "    and returns them as a dictionary\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    states (dictionary): dictionary of cities for which craigslist has a website in the format\n",
    "        {'state1':['city1', 'city2', ...], 'state2':['city3', 'city4', ...], ...}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get site\n",
    "    q = \"https://www.craigslist.org/about/sites#US\"\n",
    "    html = requests.get(q).text\n",
    "    \n",
    "    # Make beautifulsoup object\n",
    "    soup = bs(html, 'html.parser')\n",
    "    \n",
    "    # find divs on page\n",
    "    div_list = soup.find_all('div')\n",
    "    \n",
    "    # cut div_list only to the correct class of div and split out unnecessary data\n",
    "    hrefs = [p for p in div_list if p.get('class') == [u'colmask']]\n",
    "    hrefs = str(hrefs[0]).split('<h4>')\n",
    "    hrefs2 = []\n",
    "    for h in hrefs:\n",
    "        hrefs2.append(h.split('</h4>'))\n",
    "    \n",
    "    # Create states dictionary in the format {'state1':['city1_url', 'city2_url', ...], ...}\n",
    "    states = {}\n",
    "    for html_split in hrefs2:\n",
    "        if len(html_split) == 2:\n",
    "            states[html_split[0].lower()] = html_split[1]\n",
    "    \n",
    "    # Parse city urls so that the states dictionary transforms to\n",
    "    # {'state1':['city1', 'city2', ...], 'state2':['city3', 'city4', ...], ...}\n",
    "    for state in states.keys():\n",
    "        soup = bs(states[state], 'html.parser')\n",
    "        links = soup.find_all('a')\n",
    "        states[state] = [get_city_from_url(a.get(\"href\")) for a in links]\n",
    "    return states\n",
    "\n",
    "def get_car_links(url):\n",
    "    \"\"\" Gets item links from craigslist search results page\n",
    "    Runs recursively for queries with more than 1 page of results\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url (string): URL to craigslist search query page\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    test_list (list of strings): List of titles of pages\n",
    "    href_list (list of strings): List of links to pages\n",
    "    \"\"\"\n",
    "    \n",
    "    city = get_city_from_url(url)\n",
    "    \n",
    "    # Load page\n",
    "    html = requests.get(url).text\n",
    "    \n",
    "    # Create beautiful soup parser\n",
    "    soup = bs(html, 'html.parser')\n",
    "    \n",
    "    # find all links on page\n",
    "    link_list = soup.find_all('a') \n",
    "    \n",
    "    # Find all car links \n",
    "    car_list = [a for a in link_list \n",
    "                 if str(a.get('class')) == \"[u'hdrlnk']\"]\n",
    "                 #if any(x in str(a) for x in search_strings)]\n",
    "    \n",
    "    # Parse the page to get the title of and link to each item on the search results page\n",
    "    text_list, href_list = [], []\n",
    "    for l in car_list:\n",
    "        if l.contents:\n",
    "            text_list.append(unicode(l.contents[0]))\n",
    "            href = l.get('href')\n",
    "            if href.startswith(\"//\"):\n",
    "                href = \"http:{0}\".format(href)\n",
    "            elif href.startswith(\"http\"):\n",
    "                pass\n",
    "            else:\n",
    "                href = \"http://{0}.craigslist.org{1}\".format(city, href)\n",
    "            href_list.append(href)\n",
    "    \n",
    "    # Find all link tags in page - in order to move to the next page\n",
    "    link_rels = soup.find_all('link')\n",
    "    \n",
    "    # If there is a next page, then set the next_page variable and call this function (recursively) to get the next page\n",
    "    # Else return the title list and link list\n",
    "    try:\n",
    "        next_page = [a.get('href') for a in link_rels if a.get('rel')[0] == u'next'][0]\n",
    "    except IndexError:\n",
    "        return text_list, href_list\n",
    "    tl, hl = get_car_links(next_page)\n",
    "    \n",
    "    # Aggregate and return title list and link list\n",
    "    text_list+= tl\n",
    "    href_list += hl\n",
    "    return text_list, href_list\n",
    "\n",
    "def get_city_from_url(url):\n",
    "    \"\"\" Parse craigslist url to only return the city \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url (string): Any craigslist URL\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    city (string): The city from the subdomain of the URL\n",
    "    \"\"\"\n",
    "    # TODO: \n",
    "    # cshaley - 10/17/2016\n",
    "    # Make this regex instead?\n",
    "    # Add error handling?\n",
    "    return url.split('/')[2].split('.')[0]\n",
    "\n",
    "def get_attrs(lnk):\n",
    "    \"\"\" Get attributes of an object that is for sale on craigslist from the link \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lnk (string): Link to an item for sale on craigslist\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    d (dictionary): Dictionary of attributes of the item for sale on craigslist\n",
    "    \"\"\"\n",
    "    \n",
    "    # load webpage\n",
    "    html = requests.get(lnk).text\n",
    "    \n",
    "    # Create beautifulsoup parser to parse page\n",
    "    soup = bs(html, 'html.parser')\n",
    "    \n",
    "    # find all paragraphs and spans on page\n",
    "    p_list = soup.find_all('p')\n",
    "    spans = soup.find_all('span')\n",
    "    \n",
    "    # Set the price of the item from the attribute.\n",
    "    # If price is not listed, set it to NaN.\n",
    "    try:\n",
    "        price = [s for s in spans if s.get('class') == [u'price']][0].contents[0]\n",
    "    except:\n",
    "        price = np.NaN\n",
    "    \n",
    "    # Get a list of the rest of the attributes of the item\n",
    "    attrs = [p for p in p_list if p.get('class') == [u\"attrgroup\"]]\n",
    "    attr_list = attrs[1].find_all('span')\n",
    "    \n",
    "    # Create a dictionary of the attributes of the item in the format {'attribute': 'value', ...}\n",
    "    d = {}\n",
    "    d['Price'] = price\n",
    "    for attr in attr_list:\n",
    "        try:\n",
    "            d[attr.contents[0]] = attr.contents[1].contents[0]\n",
    "        except IndexError:\n",
    "            # If the attribute is listed but has no value, then ignore it\n",
    "            pass\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# General configuration steps\n",
    "# What cities do you want to search? Just the ones in your state? Or in your region?\n",
    "# Modifying state_list will automatically modify city_list\n",
    "states = get_links_by_state()\n",
    "state_list = ['texas', 'arkansas', 'alabama', 'mississippi', 'new mexico', 'colorado', 'oklahoma', 'louisiana', 'kansas']\n",
    "#state_list = ['georgia']\n",
    "city_list = []\n",
    "for state in state_list:\n",
    "    city_list += states[state]\n",
    "print city_list\n",
    "\n",
    "# Search strings and stop strings will filter your results\n",
    "# If an item's title does not contain any of the search strings, then that item will be dropped\n",
    "# If an item's title contains any of the stop strings, then that item will be dropped\n",
    "search_strings = ['2009', '2010', '2011', '2012', '2013', '2014', '2015']\n",
    "stop_strings = ['gti', 'chevrolet', 'chevy', 'ford', '3.0', 'touareg', 'nissan', 'gmc', 'chrysler',\n",
    "                'dodge', 'saturn', 'mercedes', 'honda', 'buick', 'toyota', 'jeep', 'lincoln',\n",
    "                'scion', 'yamaha', ]\n",
    "#city_list = ['dallas', 'houston', 'austin']\n",
    "#city = 'austin'\n",
    "#url = \"https://{}.craigslist.org/search/cta?query=tdi\".format(city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is where we run the meat of the program\n",
    "\n",
    "ds = [] # List of dictionaries of data\n",
    "city_list = ['dallas'] # Make city_list short for testing\n",
    "\n",
    "# TODO:\n",
    "# cshaley: 2016/10/17\n",
    "# Parallelize all this business. Why can't I seem to make URL calls work in parallel?\n",
    "\n",
    "# TODO:\n",
    "# cshaley: 2016/10/17\n",
    "# Restructure this to make it more linear.\n",
    "# i.e. Create a dataframe with all of the links to load for all of the cities,\n",
    "#      and then load all of those links.\n",
    "# Drop duplicate entries before loading them?\n",
    "\n",
    "# TODO\n",
    "# cshaley: 2016/10/18\n",
    "# Restructure the whole program - put in .py files and make it object oriented?\n",
    "\n",
    "for city in city_list:\n",
    "    # Create the query URL\n",
    "    url = \"https://{}.craigslist.org/search/cta?query=tdi\".format(city)\n",
    "    \n",
    "    # Get links to all of the items in the search results\n",
    "    tlst, hlst = get_car_links(url)\n",
    "    \n",
    "    # If there were any results\n",
    "    if tlst and hlst:\n",
    "        # Create dataframe with craigslist items for sale as rows\n",
    "        df = pd.DataFrame(zip(tlst, hlst), columns=['Title', 'Link'])\n",
    "        \n",
    "        # Filter before loading individual URLS as loading each URL is slow.\n",
    "        # Filter out items that don't contain any search strings\n",
    "        df = df[df['Title'].str.lower().str.contains('|'.join(search_strings))]\n",
    "        # Filter out items that contain stop strings\n",
    "        df = df[~df['Title'].str.lower().str.contains('|'.join(stop_strings))]\n",
    "        \n",
    "        # Load each link and get the item attributes for each link.\n",
    "        # Create a dictionary and append it t\n",
    "        for lnk, title in zip(df['Link'].values, df['Title'].values):\n",
    "            d = get_attrs(lnk)\n",
    "            d['Link'] = lnk\n",
    "            d['Title'] = title\n",
    "            ds.append(d)\n",
    "\n",
    "# Create a dataframe from the list of dictionaries\n",
    "car_data = pd.DataFrame(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print Number of results and clean data\n",
    "print(\"Raw car data number of results: {}\".format(len(car_data.index)))\n",
    "cleaned_car_data = car_data.drop_duplicates()\n",
    "print(\"Cleaned car data number of results: {}\".format(len(cleaned_car_data.index)))\n",
    "cleaned_car_data = cleaned_car_data[cleaned_car_data[u'title status: ']=='clean']\n",
    "print(\"Cleaned car data with clean title number of results: {}\".format(len(cleaned_car_data.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "car_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned_car_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write data to disk - with a timestamp so you dont overwrite anything on accident\n",
    "now = re.sub('[ .:-]', '', str(datetime.datetime.now()))\n",
    "car_data.to_csv('raw_car_data_{}.csv'.format(now), index=False, encoding='utf-8')\n",
    "cleaned_car_data.to_csv('cleaned_car_data_{}.csv'.format(now), index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read it for a sanity check\n",
    "read_car_data = pd.read_csv('raw_car_data_{}.csv'.format(now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "read_car_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
