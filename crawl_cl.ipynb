{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_links_by_state():\n",
    "    q = \"https://www.craigslist.org/about/sites#US\"\n",
    "    html = requests.get(q).text\n",
    "    soup = bs(html, 'html.parser')\n",
    "    p_list = soup.find_all('div')\n",
    "    hrefs = [p for p in p_list if p.get('class') == [u'colmask']]\n",
    "    hrefs = str(hrefs[0]).split('<h4>')\n",
    "    hrefs2 = []\n",
    "    for h in hrefs:\n",
    "        hrefs2.append(h.split('</h4>'))\n",
    "    states = {}\n",
    "    for html_split in hrefs2:\n",
    "        if len(html_split) == 2:\n",
    "            states[html_split[0].lower()] = html_split[1]\n",
    "    for state in states.keys():\n",
    "        soup = bs(states[state], 'html.parser')\n",
    "        links = soup.find_all('a')\n",
    "        states[state] = [get_city_from_url(a.get(\"href\")) for a in links]\n",
    "    return states\n",
    "\n",
    "def get_car_links(url):\n",
    "    city = get_city_from_url(url)\n",
    "    html = requests.get(url).text\n",
    "    soup = bs(html, 'html.parser')\n",
    "    link_list = soup.find_all('a') \n",
    "    car_list = [a for a in soup.find_all('a') \n",
    "                 if str(a.get('class')) == \"[u'hdrlnk']\"]\n",
    "                 #if any(x in str(a) for x in search_strings)]\n",
    "    text_list, href_list = [], []\n",
    "    for i, l in enumerate(car_list):\n",
    "        text_list.append(unicode(l.contents[0]))\n",
    "        href = l.get('href')\n",
    "        if href.startswith(\"//\"):\n",
    "            href = \"http:{0}\".format(href)\n",
    "        elif href.startswith(\"http\"):\n",
    "            pass\n",
    "        else:\n",
    "            href = \"http://{0}.craigslist.org{1}\".format(city, href)\n",
    "        href_list.append(href)\n",
    "    link_rels = soup.find_all('link')\n",
    "    try:\n",
    "        next_page = [a.get('href') for a in link_rels if a.get('rel')[0] == u'next'][0]\n",
    "    except IndexError:\n",
    "        return text_list, href_list\n",
    "    tl, hl = get_car_links(next_page)\n",
    "    text_list+= tl\n",
    "    href_list += hl\n",
    "    return text_list, href_list\n",
    "\n",
    "def get_city_from_url(url):\n",
    "    return url.split('/')[2].split('.')[0]\n",
    "\n",
    "def get_attrs(lnk):\n",
    "    html = requests.get(lnk).text\n",
    "    soup = bs(html, 'html.parser')\n",
    "    p_list = soup.find_all('p')\n",
    "    spans = soup.find_all('span')\n",
    "    try:\n",
    "        price = [s for s in spans if s.get('class') == [u'price']][0].contents[0]\n",
    "    except:\n",
    "        price = np.NaN\n",
    "    attrs = [p for p in p_list if p.get('class') == [u\"attrgroup\"]]\n",
    "    attr_list = attrs[1].find_all('span')\n",
    "    d = {}\n",
    "    d['Price'] = price\n",
    "    for attr in attr_list:\n",
    "        try:\n",
    "            d[attr.contents[0]] = attr.contents[1].contents[0]\n",
    "        except IndexError:\n",
    "            pass\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'albanyga', u'athensga', u'atlanta', u'augusta', u'brunswick', u'columbusga', u'macon', u'nwga', u'savannah', u'statesboro', u'valdosta']\n"
     ]
    }
   ],
   "source": [
    "states = get_links_by_state()\n",
    "#state_list = ['texas', 'arkansas', 'alabama', 'mississippi', 'new mexico', 'colorado', 'oklahoma', 'louisiana', 'kansas']\n",
    "state_list = ['georgia']\n",
    "city_list = []\n",
    "for state in state_list:\n",
    "    city_list += states[state]\n",
    "print city_list\n",
    "search_strings = ['2009', '2010', '2011', '2012', '2013', '2014', '2015']\n",
    "stop_strings = ['gti', 'chevrolet', 'chevy', 'ford', '3.0', 'touareg', 'nissan', 'gmc', 'chrysler',\n",
    "                'dodge', 'saturn', 'mercedes', 'honda', 'buick', 'toyota', 'jeep', 'lincoln',\n",
    "                'scion', 'yamaha', ]\n",
    "#city_list = ['dallas', 'houston', 'austin']\n",
    "#city = 'austin'\n",
    "#url = \"https://{}.craigslist.org/search/cta?query=tdi\".format(city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = []\n",
    "city_list = ['dallas']\n",
    "for city in city_list:\n",
    "    url = \"https://{}.craigslist.org/search/cta?query=tdi\".format(city)\n",
    "    tlst, hlst = get_car_links(url)\n",
    "    if tlst and hlst:\n",
    "        df = pd.DataFrame(zip(tlst, hlst), columns=['Title', 'Link'])\n",
    "        df = df[df['Title'].str.lower().str.contains('|'.join(search_strings))]\n",
    "        df = df[~df['Title'].str.lower().str.contains('|'.join(stop_strings))]\n",
    "        for lnk, title in zip(df['Link'].values, df['Title'].values):\n",
    "            d = get_attrs(lnk)\n",
    "            d['Link'] = lnk\n",
    "            d['Title'] = title\n",
    "            ds.append(d)\n",
    "car_data = pd.DataFrame(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(car_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "car_data.to_csv('raw_car_data.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "car_data = pd.read_csv('raw_car_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_car_data = car_data.drop_duplicates()\n",
    "print(len(cleaned_car_data.index))\n",
    "cleaned_car_data = cleaned_car_data[cleaned_car_data[u'title status: ']=='clean']\n",
    "len(cleaned_car_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned_car_data.to_csv('cleaned_car_data.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
